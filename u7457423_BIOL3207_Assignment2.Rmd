---
title: "BIOL3207_Assignment2"
author: "Yusheng_Wang_u7457423"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=2)
```
# Git Hub Repo

[My GitHub Repository](https://github.com/moaaom/YUSHENG_BIOL3207_ASSIGNMENT2.git)

# Data Cleansing

## Load Packages and Data

First, let's load some useful packages.
```{r loadpackages, message = F, results='hide'}
# load packages with library()
library(bookdown)
library(tidyverse)
library(metafor)
library(orchaRd)
library(outliers)
library(patchwork)
library(flextable)
```

Then, let's read data.
```{r readdata, message = F}
# read raw data from Clark et al. (2020)
crdata <- read_csv ("./raw_data/OA_activitydat_20190302_BIOL3207.csv")

# read metadata from Clark et al. (2020)
cmrdata <- read_csv("./raw_data/clark_paper_data.csv")

# read meta-analysis dataset
mdata <- read_csv("./raw_data/ocean_meta_data.csv")
```

## Data Analyses for Clark *et al.* (2020)

First, let's remove rows that include NA values. Because they are useless that we cannot guess what the values actually are.
```{r rmna, message = F}
# check the completeness of values with complete.cases() and remove NA with filter()
ncrdata <- crdata %>% filter(., complete.cases(.))

# have a quick look
head(ncrdata)
```

Then, let's drop out irrelevant columns. This time, we only need the column `species`, `treatment` and `activity`.
```{r rmirre}
# select and keep the three columns
cdata <- tibble(species = ncrdata$species, treatment = ncrdata$treatment, activity = ncrdata$activity)

# have a quick look
head(cdata)
```

classes of columns `species` and `treatment` should be factors. Let's change them.
```{r refactors, message = F}
# change the last three column to factors
cdata$species <- as.factor(cdata$species)
cdata$treatment <- as.factor(cdata$treatment)

# have a quick look
head(cdata)
```

In this tibble, the columns, from left to right, are:
```{r colmeaning, message = F, eval = F}
1) Species name: acantho = Acanthochromis polyacanthus; Ambon = Pomacentrus amboinensis; Chromis = Chromis atripectoralis; Humbug = Dascyllus aruanus; Lemon = Pomacentrus moluccensis; whitedams = Dischistodus perspicillatus
2) Elevated CO2 [CO2] (850-1,050 µatm) or control [Control] (400 - 450 µatm) groups
3) Number of seconds the fish was active per minute, averaged across the duration of the trial
```

Then, let's check the structure of the data to find whether there are some spelling issues in columns `species` and `treatment`.
```{r checkspelling, message = F}
# show the internal structure with str()
str(cdata)
```
Right! We have 6 species and 2 treatment.

Then, let's check whether or not there are some issues in column `activity`. We can plot box plots to see the outliers for each species.
```{r checkactivity, message = F, fig.cap = "Box plots for activity of each species."}
# plot box plot for each species
ggplot(cdata, aes(x = species, y = activity, col = species)) +
  geom_boxplot()
```

It looks great! Although there are some outliers in Figure \@ref(fig:checkactivity), they do not look like mistakes.

Now, we can create a table of summary data that includes: 1) the mean, 2) the standard deviation and 3) the sample sizes of unique fish across all six fish species for each treatment.
```{r summarytab, message = F, tab.cap= "Summary of activity data from Clark et al. (2020)"}
# calculate means, SD and N by species and treatment
cdata_group <- cdata %>% group_by(species, treatment) %>% mutate(size = length(activity), mean_activity = mean(activity), sd_activity = sd(activity))

# delete duplicate data and sort
cdata_summary <- cdata_group %>% .[, c(-3)] %>% .[!duplicated(.), ] %>% .[order(.$species, .$treatment), ]

# use flextable to render the summary table in a tidy format
flextable(cdata_summary) %>% add_footer_lines(., values = c("size, sample size", "mean_activity, mean of activity", "sd_activity, standard deviation of activity")) 
```

## Metadata Formatting

Now, we start to merge the data from Clark et al. (2020) with the "ocean_meta_data.csv".

First, let's re-write the Table \@ref(tab:summarytab) to one species in one row.
```{r widesummarytab, message = F}
# write a new tibble with pivot_wider() to divide the old tibble by treatment
ncdata_summary <- cdata_summary %>% pivot_wider(id_cols = species, names_from = treatment, values_from = c(size, mean_activity, sd_activity))

# have a quick look
ncdata_summary
```

Looks great! Now, let's give the full species name, re-order and re-name these columns to meet format of the metadata file.
```{r resummarytab, message = F, tab.cap= "Wider summary of activity data from Clark et al. (2020)"}
# give the full species name
ncdata_summary$species <- c("Acanthochromis polyacanthus", "Pomacentrus amboinensis", "Chromis atripectoralis", "Dascyllus aruanus", "Pomacentrus moluccensis", "Dischistodus perspicillatus")

# re-order the tibble
ncdata_summary <- ncdata_summary[, c(1, 3, 5, 7, 2, 4, 6)]

# re-name the tibble
names(ncdata_summary) <- c("Species", "ctrl.n", "ctrl.mean", "ctrl.sd", "oa.n", "oa.mean", "oa.sd")

# have a quick look with flextable()
flextable(ncdata_summary) %>% add_footer_lines(., values = c("ctrl.n, sample size of control group", "ctrl.mean, mean activity of control group", "ctrl.sd, standard deviation of activity of control group", "oa.n, sample size of treatment group", "oa.mean, mean activity of treatment group", "oa.sd, standard deviation of activity of treatment group"))
```


Then, we can write the summary statistics in the Table \@ref(tab:resummarytab) into the metadata format.
```{r mergeClark, message = F}
# because we have 6 species and 2 treatments, we need 12 rows to store the summary data
cmdata <- cmrdata
cmdata[1:12, ] <- cmrdata[1, ]

# bind the new tibble
fcdata <- cbind(cmdata, ncdata_summary)
```

Finally, let's add the final tibble into the "ocean_meta_data.csv".
```{r mergefinal, message = F}
# add the tibble with marge() to let the column names to automatch
data <- rbind(mdata, fcdata)

# have a quick look
tail(data, 15)
```

Looks good! But there are still some small problems:

1. column names have spaces. Let's rename them.
```{r renamecol, message = F}
# rename the columns
colnames(data) <- gsub(" ", "_", colnames(data))

# have a quick look
data
```

2. Entries of species names have spaces. Let's rename them again.Less spaces is always better.
```{r renamesp, message = F}
# replace spaces by underline with gsub()
data$Species <- gsub(" ", "_", data$Species)

# have a quick look
head(data)
```

3. There are some "-" in column `Pub_year_IF`, `2017_IF` and `Cue/stimulus_type`. Let's replace them by `NA`.
```{r replace-, message = F}
# replace - by NA with gsub()
data$Pub_year_IF <- gsub("-", NA, data$Pub_year_IF)
data$`2017_IF` <- gsub("-", NA, data$`2017_IF`)
data$`Cue/stimulus_type` <- gsub("-", NA, data$`Cue/stimulus_type`)
```

4. Add a column `observation` to mark each row.
```{r obs, message = F}
# add a column of row numbers
data <- tibble(Obs = 1 : nrow(data), data)

# have a quick look
head(data)
```

5. Change the classes of `Year_(online)`, `Year_(print)`, `Obs` and `Study` columns to factor
```{r class, message = F}
# change to factors
data$`Year_(online)` <-as.factor(data$`Year_(online)`)
data$`Year_(print)` <-as.factor(data$`Year_(print)`)
data$Obs <-as.factor(data$Obs)
data$Study <-as.factor(data$Study)

# have a quick look
head(data)
```

Good job! Now, we finish our data cleansing and can output the new metadata file.
```{r outputdata, message = F}
# output a .csv file
write_csv(data, "./out_data/metadata.csv")
```

# Meta-analysis

## Overall Meta-analytis and Heterogeneity Analysis

First, let's calculate the effect sizes. We will use the log response ratio, which is ‘ROM’ in Metafor.
```{r, RRcal, message = F}
# calculate log response ratio (ROM), where should be given the mean, sample size and standard deviation of experiment and control groups.
InRR <- escalc(measure= "ROM", m1i = oa.mean, sd1i = oa.sd, n1i = oa.n, m2i = ctrl.mean, sd2i = ctrl.sd, n2i = ctrl.n, data = data, var.names = c("InRR", "InRR_V"))

# have a quick look
head(InRR)
```

Oops! We have some NaNs. This is because there are some negative value in `ctrl.mean` and `oa.mean`, which cause the logarithm to fail to be calculated. They are  meaningless to our the rest study. Let's remove them.
```{r nanrm, message = F}
# remove rows containing NA
cInRR <- InRR %>% filter(., complete.cases(.$InRR) & complete.cases(.$InRR_V))
```

Let's make some box plot to see whether or not there are some outliers.
```{r outlier, message = F, fig.cap = "a) Box plot for log response ratio (InRR). b) Box plot for sampling variance of lnRR. There are some extreme outliers in b)"}
# plot box plot for InRR
p1 <- ggplot(cInRR, aes(x = InRR))+
  geom_boxplot()+
  ggtitle("a)")

# plot box plot for InRR_V
p2 <- ggplot(cInRR, aes(x = InRR_V))+
  geom_boxplot()+
  ggtitle("b)")

# bind the two plots
p1 + p2
```

It looks like there are many extreme outliers in plot b) of the Figure \@ref(fig:outlier). But, that's what meta-analysis does, finding the gaps in previous experiments and analyzing them again. So, we can maintain these outliers.

It time to out put our meta-analysis data. In which the column `InRR` is log response ratio, and column `InRR_V` is sampling variance of lnRR. 
```{r outputmetadata, message = F}
# output a .csv file
write_csv(cInRR, "./out_data/InRR_metadata.csv")
```

Now, we start to use a multilevel meta-analytic model.
```{r MLMA, message = F, warning = F}
# do a multilevel meta-analytic model with random effect of study and observation
MLMA <- rma.mv(yi = InRR ~ 1, V = InRR_V,
               # via restricted maximum likelihood
               method = "REML",
               # estimate a random effect variance between study and observation
               random = list(~1 | Study,
                             ~1 | Obs),
               # use t-test rather than z-test
               test = "t",
               # calculate the degrees of freedom based on the lowest clustering level
               dfs = "contain",
               data = cInRR)
MLMA
```

Now, we start to measure the heterogeneity in effect size estimates across studies.
```{r I2, message = F}
# calculate I^2 by i2_ml()
i2 <- i2_ml(MLMA, data = cInRR)
i2
```

Then, we calculate the prediction interval for heterogeneity
```{r pi, message = F}
# calculate prediction interval by predict()
preMLMA <- predict(MLMA)
preMLMA
```

Let's make a overall forest plot to show the statistics above visually. Because we need a overall mean estimate for InRR, we only estimate the `intercept`.
```{r forest1, message = F, fig.cap = "Orchard plot showing the mean InRR. k = the number of effect sizes and the number of studies are in brackets. The number of samples is shown in blue circle"}
# use orchard_plot() to make a modified forest plot
# argument mod = "1" means calculate the intercept, argument N = "Average_n" means show the sample size instead of precision
orchard_plot(MLMA, mod = "1", group = "Study", data = cInRR, xlab = "Log Response Ratio (InRR)", angle = 45, N = "Average_n")
```

Now, we can do a summary of these overall meta-analysis above:

1. From the multilevel meta-analytic model, the overall meta-analytic mean is `r coef(MLMA)`, which suggests that for every 1 unit increase in ocean acidification, the fish behavior measurement increases by `r coef(MLMA)` unit. This mean can be supported with the Figure \@ref(fig:forest1) that the black hollow circle representing the mean is very close to the vertical dotted line (0 of InRR).

2. From the multilevel meta-analytic model, the 95% confidence interval is `r MLMA$ci.lb` to `r MLMA$ci.ub`, which suggests that we are 95% confident that the true mean falls between `r MLMA$ci.lb` and `r MLMA$ci.ub`.
 
3. From the heterogeneity analysis, we do have a significant amount of heterogeneity among effects (Q = `r MLMA$QE`, df = 806, p = <0.001), with effect sizes expected to be as low as `r preMLMA$pi.lb` to as high as `r preMLMA$pi.ub`. This interval interval can also be supported with the Figure \@ref(fig:forest1) that the horizontal solid black line representing the 95% confidence interval is very close to the vertical dotted line (0 of InRR).

4. From the heterogeneity analysis, 95% of the time (*I<sup>2</sup>*total = `r i2[1]`). Differences among study and obvious explain `r i2[2]` and `r i2[3]` of effect size variation, respectively.

We can make more forest plots with other moderators:

1. Use `Year_.online.` as a moderator. We can test the correlation between publication year and InRR with multilevel meta-regression.
```{r ymMLMR, message = F, warning = F}
# do multilevel meta-regression
ymMLMR <- rma.mv(InRR ~ Year_.online., V = InRR_V, 
                random = list(~1 | Study,
                              ~1 | Obs),
                test = "t",
                dfs = "contain",
                data = yInRR)
ymMLMR
```

Then, make a forest plot to show the statistics above visually.
```{r forest2, message = F, fig.cap = "Orchard plot showing the mean InRR of each publication year. k = the number of effect sizes and the number of studies are in brackets. The number of samples is shown in blue circle"}
# use orchard_plot() to make a modified forest plot
# argument N = "Average_n" means show the sample size instead of precision
orchard_plot(ymMLMR, mod = "Year_.online.", group = "Study", data = cInRR, xlab = "Log Response Ratio (InRR)", angle = 45, N = "Average_n")
```

We can see from the Figure \@ref(fig:forest2), the black hollow circle representing the mean InRR of year `2009` and `2010` are relatively far from the vertical dotted line (0 of InRR). This could suggest that early studies appear to have a far higher effect size.

Finally, we calculate how many variation does time when results were published explain in InRR.
```{r r2_y, message = F}
# calculate heterogeneity
r2_y <- r2_ml(ymMLMR)
r2_y
```

The publication year explains `r r2_y[1]` of the variation in InRR. As we got from the Figure \@ref(fig:forest2), there could be a correlation between InRR and publication year. We will test it again in details in publication bias part below.


2. Use `Climate_.FishBase.` as a moderator. We can test the correlation between different climatic region and InRR with multilevel meta-regression.
```{r smMLMR, message = F, warning = F}
# do multilevel meta-regression
cmMLMR <- rma.mv(InRR ~ Climate_.FishBase., V = InRR_V, 
                random = list(~1 | Study,
                              ~1 | Obs),
                test = "t",
                dfs = "contain",
                data = yInRR)
```

Then, make a forest plot to show the statistics above visually.
```{r forest3, message = F, fig.cap = "Orchard plot showing the mean InRR of impact factor. k = the number of effect sizes and the number of studies are in brackets. The number of samples is shown in blue circle"}
# use orchard_plot() to make a modified forest plot
# argument N = "Average_n" means show the sample size instead of precision
orchard_plot(cmMLMR, mod = "Climate_.FishBase.", group = "Study", data = cInRR, xlab = "Log Response Ratio (InRR)", angle = 45, N = "Average_n")
```

We can see from the Figure \@ref(fig:forest3), the black hollow circle representing the mean InRR of climatic region `Deep` and `Trop` are relatively far from the vertical dotted line (0 of InRR). This could suggest that fishes in the two region are more likely to be affected by ocean acidification.

Finally, we calculate how many variation does different regions explain in InRR.
```{r r2_c, message = F}
# calculate heterogeneity
r2_c <- r2_ml(cmMLMR)
r2_c
```

The climatic region explains `r r2_c[1]` of the variation in InRR. As we got from the Figure \@ref(fig:forest3), there could be a correlation between InRR and climatic region.

3. Use `Life_stage` as a moderator. We can test the correlation between different life stage and InRR with multilevel meta-regression.
```{r lmMLMR, message = F, warning = F}
# do multilevel meta-regression
lmMLMR <- rma.mv(InRR ~ Life_stage, V = InRR_V, 
                random = list(~1 | Study,
                              ~1 | Obs),
                test = "t",
                dfs = "contain",
                data = yInRR)

```

Then, make a forest plot to show the statistics above visually.
```{r forest4, message = F, fig.cap = "Orchard plot showing the mean InRR of impact factor. k = the number of effect sizes and the number of studies are in brackets. The number of samples is shown in blue circle"}
# use orchard_plot() to make a modified forest plot
# argument N = "Average_n" means show the sample size instead of precision
orchard_plot(lmMLMR, mod = "Life_stage", group = "Study", data = cInRR, xlab = "Log Response Ratio (InRR)", angle = 45, N = "Average_n")
```

We can see from the Figure \@ref(fig:forest4), the black hollow circle representing the mean InRR of life stage `Larvae` is relatively far from the vertical dotted line (0 of InRR). This could suggest that fish larvae is more likely to be affected by ocean acidification.

Finally, we calculate how many variation does different regions explain in InRR.
```{r r2_l, message = F}
# calculate heterogeneity
r2_l <- r2_ml(lmMLMR)
r2_l
```

The life stage explains `r r2_l[1]` of the variation in InRR. As we got from the Figure \@ref(fig:forest4), there could be a correlation between InRR and life stage.

4. Use `Journal` as a moderator. We can test the correlation between different journals and InRR with multilevel meta-regression.
```{r jmMLMR, message = F, warning = F}
# do multilevel meta-regression
jmMLMR <- rma.mv(InRR ~ Journal, V = InRR_V, 
                random = list(~1 | Study,
                              ~1 | Obs),
                test = "t",
                dfs = "contain",
                data = yInRR)

```

Because there are so many journals, we do not make a forest plot again.

Finally, we calculate how many variation does different journals explain in InRR.
```{r r2_j, message = F}
# calculate heterogeneity
r2_j <- r2_ml(jmMLMR)
r2_j
```

The life stage explains `r r2_j[1]` of the variation in InRR. So, there could be a correlation between InRR and different journals. Some journals could be more likely publish some kinds of studies (e.g. high effect size).

5. Use `Species` as a moderator. We can test the correlation between different species and InRR with multilevel meta-regression.
```{r smMLMR, message = F, warning = F}
# do multilevel meta-regression
smMLMR <- rma.mv(InRR ~ Species, V = InRR_V, 
                random = list(~1 | Study,
                              ~1 | Obs),
                test = "t",
                dfs = "contain",
                data = yInRR)

```

Because there are so many species, we do not make a forest plot again.

Finally, we calculate how many variation does different species explain in InRR.
```{r r2_j, message = F}
# calculate heterogeneity
r2_s <- r2_ml(smMLMR)
r2_s
```

The life stage explains `r r2_s[1]` of the variation in InRR. So, there could be a correlation between InRR and different species. Some species could be more sensitive to ocean acidification.

## Publication Biases

Now, we start to find the publication biases. The first thing is make a funnel plot for visually assessing the possibility of publication bias.
```{r funnel, message = F, fig.cap = "Funnel plot depicting the correlation between metabolism and fitness as a function of precision (1 / SE). The dotted lines are the theoretical 95% sampling variance intervals - the interval with which we expect effect size estimates to fall within if only sampling variance drives differences in effects. Shaded regions represent the p-value of studies. The white region indicates studies where the p-value is between 0.1 and 1; dark gray where the p-value of studies is between 0.05 and 0.1 and the lighter gray regions where the p-value of studies is significant."}
# plot a funnel plot
funnel(x = cInRR$InRR, vi = cInRR$InRR_V,
       # for the inverse of the standard errors
       yaxis = "seinv", 
       # specify the number of decimal places
       digits = 2, 
       # specify the level of the confidence interval
       level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray 75"), las = 1, 
       # for aesthetics
       xlab = "Log Response Ratio (InRR)", ylim = c(0.1,5), legend = TRUE)
```

Now, we see some hint from the Figure \@ref(fig:funnel):

1. There are relatively less points in the bottom right corner than bottom left corner. This suggests that researches that show ocean acidification slightly make the fish more active are less likely to be published.

2. There are relativel more points in the right part with low Inverse Standard Error than left part. This suggest researches that show ocean acidification significantly make the fish more active are easy to be published, although they have low sample sizes.

The next step is test for time-lag bias.

First, let's centre the `Year_.online.` variable by subtracting every value of `Year_.online.` by the `mean year`.
```{r cyear, message = F}
# centre the Year_.online.
yInRR <- cInRR %>% mutate(Year_c = as.numeric(Year_.online.) - mean(as.numeric(Year_.online.)))

yInRR <- yInRR %>% group_by(Year_.online.) %>% mutate(ymean = mean(InRR))

# have a quicl look
head(yInRR)
```

Let's make a plot to see whether or not mean InRRs change with publication year.
```{r year, message = F, fig.cap = "Relationship between InRR and the year of publication. Points are scaled in relation to their precision (1/sqrt(InRR_V)). Small points indicate effects with low precision or high sampling varaince"}
# use online publication time as year of publication
ggplot(yInRR, aes(x = Year_.online., y = InRR)) +
  # make a point plot with different size of different precision
  geom_point(alpha = 0.3, aes(size = 1/sqrt(InRR_V))) +
  # make a regression line with linear model
  geom_smooth(method = "lm", col = "red") +
  # for aesthetics
  labs(x = "Publication Year", y = "Log Response Ratio (InRR)", size = "Precision (1/SE)") + theme_classic()
```

We see some hint from the Figure \@ref(fig:year):

1. It seems like there is a slight correlation between publication year and InRR.

2. These early studies appear to have a far higher effect size (InRR) compared with studies that are done in later years.

Now, we test the correlation between publication year and InRR with multilevel meta-regression.
```{r yMLMR, message = F, warning = F}
# do multilevel meta-regression
yMLMR <- rma.mv(InRR ~ Year_c + InRR_V, V = InRR_V, 
                random = list(~1 | Study,
                              ~1 | Obs),
                test = "t",
                dfs = "contain",
                data = yInRR)
```

Now, we calculate how many variation does time when results were published explain in InRR.
```{r r2_year, message = F}
# calculate heterogeneity
r2_year <- r2_ml(yMLMR)
r2_year
```
The time-lag explains `r r2_year[1]` of the variation in InRR. As we got from the Figure \@ref(fig:year), there is not a strong evidence for time-lag bias.

To show whether or not there is a file-drawer bias between InRR and precision (inverse sampling variance), we make a plot to see the correlation between InRR and precision.
```{r precision, message = F, warning = F, fig.cap = "Relationship between InRR and precision (1/vInRR)."}
ggplot(yInRR, aes(x = (1 / InRR_V), y = InRR)) +
  # make a point plot with different size of different precision
  geom_point(alpha = 0.3) +
  # make a regression line with linear model
  geom_smooth(method = "lm", col = "red") +
  # for aesthetics
  labs(x = "Precision (1/vInRR)", y = "Log Response Ratio (InRR)") + theme_classic() + xlim(0,100)
```

We see from the Figure \@ref(fig:precision) that there are many values appear in the upper left and lower left corners of the figure, which have low precision. It suggests that some studies showing big effect size sometime also have a big sampling variance. However, regardless of accuracy, most studies show that InRR is close to 0 (no effect).

Then, we can do multilevel meta-regression again to test the correlation.
```{r pMLMR, message = F, warning = F}
# do multilevel meta-regression
pMLMR <- rma.mv(InRR ~ (1 / InRR_V), V = InRR_V, 
                random = list(~1 | Study,
                              ~1 | Obs),
                test = "t",
                dfs = "contain",
                data = yInRR)
```

Then, we calculate how many variation does precision explain in InRR.
```{r r2_precision, message = F}
# calculate heterogeneity
r2_precision <- r2_ml(pMLMR)
r2_precision
```
The precision explains `r r2_precision[1]` of the variation in InRR. As we got from the Figure \@ref(fig:precision), there is no strong correlation between InRR and precision.

We can make more analyses with another moderator to see the potential publication bias:

Use `Pub_year_IF` as a moderator. We can test the correlation between impact factors and InRR with multilevel meta-regression.

First, let's make a plot to see whether or not mean InRRs change with impact factors.
```{r if, message = F, fig.width = 20 ,fig.cap = "Relationship between InRR and different journals. Points are scaled in relation to their precision (1/sqrt(InRR_V)). Small points indicate effects with low precision or high sampling varaince"}
# use online publication time as year of publication
ggplot(yInRR, aes(x = Pub_year_IF, y = InRR)) +
  # make a point plot with different size of different precision
  geom_point(alpha = 0.3, aes(size = 1/sqrt(InRR_V))) +
  # make a regression line with linear model
  geom_smooth(method = "lm", col = "red") +
  # for aesthetics
  labs(x = "Publication Year", y = "Log Response Ratio (InRR)", size = "Precision (1/SE)") + theme_classic()
```

We can see some hint from the Figure \@ref(fig:if): Some journals with high impact factors prefer to publish studies with large effect sizes, even though their precision are small.

Now, we test the correlation between impact factors and InRR with multilevel meta-regression.
```{r iMLMR, message = F, warning = F}
# do multilevel meta-regression
iMLMR <- rma.mv(InRR ~ Pub_year_IF + InRR_V, V = InRR_V, 
                random = list(~1 | Study,
                              ~1 | Obs),
                test = "t",
                dfs = "contain",
                data = yInRR)
```

Then, we calculate how many variation does time when results were published explain in InRR.
```{r r2_if, message = F}
# calculate heterogeneity
r2_if <- r2_ml(iMLMR)
r2_if
```
The time-lag explains `r r2_if[1]` of the variation in InRR. As we got from the Figure \@ref(fig:if), there is a strong evidence for publication bias that some journals are more likely to publish studies with high effect size.

Up to now, we can make a summary for out publication biases analyses.

1. About the p-value of studies, we can see in the Figure \@ref(fig:funnel):

1. There are relatively less points in the bottom right corner than bottom left corner. This suggests that researches that show ocean acidification slightly make the fish more active are less likely to be published.

2. There are relativel more points in the right part with low Inverse Standard Error than left part. This suggest researches that show ocean acidification significantly make the fish more active are easy to be published, although they have low sample sizes.















